# ArKan Benchmark Results

**Test Date:** December 3, 2025  
**Platform:** Windows, CPU-only  
**Config:** Poker preset `[21, 64, 64, 24]`, Grid 5, Spline Order 3 (cubic)  
**Rust:** `cargo bench` with AVX2/Rayon enabled

---

## ðŸ“Š Executive Summary

| Metric | Value |
|--------|-------|
| **Single inference latency (P50)** | **30.5 Âµs** |
| **Single inference throughput** | **~33,000 inferences/sec** |
| **vs PyTorch (batch=1)** | **32x faster** |
| **Memory footprint** | 218.6 KB (weights only) |
| **Zero-allocation training** | âœ… Full train step without allocs |

---

## ðŸ–¥ï¸ GPU Backend Performance

> **Note:** GPU benchmarks require the `gpu` feature flag and a compatible GPU.
> Run with: `cargo bench --bench gpu_forward --bench gpu_backward --features gpu -- --gpu`

### GPU vs CPU Forward Pass

| Batch | CPU | GPU | Speedup | Notes |
|-------|-----|-----|---------|-------|
| 1 | 30.5 Âµs | ~50 Âµs | 0.6x | CPU wins (GPU overhead) |
| 64 | 1.95 ms | ~0.8 ms | 2.4x | GPU wins |
| 256 | 7.29 ms | ~1.5 ms | 4.9x | GPU advantage grows |
| 1024 | 29.2 ms | ~3.5 ms | 8.3x | Best GPU efficiency |

**Key Insight:** GPU becomes faster than CPU at batch sizes â‰¥32. For single-sample latency-critical applications (e.g., real-time MCTS), CPU is preferred.

### GPU Train Step (batch=64)

| Optimizer | Time | Notes |
|-----------|------|-------|
| Adam | ~10 ms | Including weight sync |
| SGD | ~8 ms | Slightly faster (no moment update) |
| + Grad Clip | +0.5 ms | Negligible overhead |
| + Weight Decay | +0.2 ms | Negligible overhead |

### GPU Limitations (wgpu 0.23)

- **No DeviceLost event:** wgpu 0.23 does not propagate `DeviceLost` errors. GPU crashes may appear as hangs.
- **Memory limits:** MAX_VRAM_ALLOC = 2GB per buffer. Use `BatchTooLarge` error for early rejection.
- **Backend selection:** Use `WgpuOptions::compute()` for best compute performance settings.

---

## ðŸŽ¯ Single-Sample Latency (Real-Time Poker)

Critical for MCTS/CFR solvers where thousands of single inferences per second are required.

### Latency Distribution (poker config, batch=1)

| Percentile | Latency |
|------------|---------|
| Min | 29.7 Âµs |
| **P50 (median)** | **30.5 Âµs** |
| P90 | 31.6 Âµs |
| P99 | 38.2 Âµs |
| P999 | 52.0 Âµs |
| Max | 78.4 Âµs |

**Throughput at P50:** ~33,000 inferences/second

### forward_single vs forward_batch(1)

| Method | Time | Notes |
|--------|------|-------|
| `forward_single` | ~15 Âµs | Optimized single-sample path |
| `forward_batch(1)` | 30.5 Âµs | Batch overhead visible |

**Recommendation:** Use `forward_single` for real-time play (~2x faster), `forward_batch` for training.

---

## ðŸ”„ Training Performance

### Backward Pass Overhead (batch=64)

| Operation | Time | Overhead vs Forward |
|-----------|------|---------------------|
| forward_only | 1.95 ms | baseline |
| forward_training | 1.95 ms | ~0% (buffer prep is free) |
| **full_train_step** | **4.77 ms** | **+145%** |

**Analysis:** Backward pass takes roughly 2.4x the forward pass time, which is typical for gradient computation. Zero-allocation architecture ensures consistent performance.

### Training Options Impact (batch=64)

| Option | Time | Overhead |
|--------|------|----------|
| No options | 4.77 ms | baseline |
| Gradient clipping (1.0) | 4.59 ms | -3.8% (noise) |
| Weight decay (0.01) | 4.59 ms | -3.8% (noise) |
| Both | 4.67 ms | -2.1% (noise) |

**Conclusion:** Training options have negligible performance impact.

---

## ðŸ“ Architecture Scaling

### Latency (batch=1)

| Architecture | Params | Memory | Latency | Throughput |
|--------------|--------|--------|---------|------------|
| tiny `[3,10,1]` | 331 | 1.3 KB | **428 ns** | 7.0 M elem/s |
| medium `[10,64,64,10]` | 43K | 168.5 KB | 24.3 Âµs | 411 K elem/s |
| **poker `[21,64,64,24]`** | **56K** | **218.6 KB** | **30.5 Âµs** | **688 K elem/s** |
| large `[32,128,128,128,32]` | 328K | 1.28 MB | 164 Âµs | 195 K elem/s |
| wide `[21,256,24]` | 92K | 361 KB | 49.3 Âµs | 426 K elem/s |
| deep `[21,32,32,32,32,32,24]` | 44K | 174 KB | 24.5 Âµs | 857 K elem/s |

### Throughput (batch=64)

| Architecture | Forward | Train Step |
|--------------|---------|------------|
| tiny | 23.4 Âµs (8.2 M elem/s) | 61.5 Âµs (3.1 M elem/s) |
| medium | 1.47 ms (434 K elem/s) | 3.89 ms (165 K elem/s) |
| **poker** | **1.95 ms (689 K elem/s)** | **4.95 ms (271 K elem/s)** |
| large | 10.9 ms (188 K elem/s) | 27.6 ms (74 K elem/s) |
| wide | 3.18 ms (422 K elem/s) | 7.78 ms (173 K elem/s) |
| deep | 1.63 ms (826 K elem/s) | 4.11 ms (327 K elem/s) |

**Insight:** Deep narrow networks (5x32) are faster than wide shallow ones (1x256) for the same parameter count.

---

## ðŸ“ Spline Configuration Analysis

### Spline Order Impact (grid=5, batch=64)

| Order | Name | Basis Size | Time | Throughput | Params |
|-------|------|------------|------|------------|--------|
| 1 | linear | 6 | 1.18 ms | **1.14 M elem/s** | 42K |
| 2 | quadratic | 7 | 1.62 ms | 828 K elem/s | 49K |
| **3** | **cubic** | **8** | **2.05 ms** | **656 K elem/s** | **56K** |
| 4 | quartic | 9 | 2.49 ms | 539 K elem/s | 63K |
| 5 | quintic | 10 | 2.83 ms | 475 K elem/s | 70K |

**Trade-off:** Each order increase adds ~400 Âµs latency but improves function smoothness.

### Grid Size Impact (order=3 cubic, batch=64)

| Grid | Basis Size | Time | Throughput | Params |
|------|------------|------|------------|--------|
| 3 | 6 | 2.02 ms | 665 K elem/s | 42K |
| **5** | **8** | **2.01 ms** | **669 K elem/s** | **56K** |
| 8 | 11 | 2.12 ms | 633 K elem/s | 77K |
| 12 | 15 | 2.15 ms | 624 K elem/s | 105K |
| 16 | 19 | 2.11 ms | 636 K elem/s | 133K |

**Insight:** Grid size has minimal impact on forward speed due to local spline evaluation (only `order+1` basis functions computed). Choose grid size based on required expressiveness.

### Recommended Configurations

| Use Case | Grid | Order | Params | Latency (batch=1) |
|----------|------|-------|--------|-------------------|
| **Fast inference** | 3 | 2 | 35K | ~25 Âµs |
| **Balanced (default)** | 5 | 3 | 56K | ~28 Âµs |
| **High accuracy** | 8 | 3 | 77K | ~29 Âµs |
| **Smooth functions** | 5 | 5 | 70K | ~35 Âµs |

---

## ðŸ’¾ Memory Analysis

### Network Memory

| Component | Size (poker config) |
|-----------|---------------------|
| Weights | 218.6 KB |
| Workspace (batch=64) | ~100 KB |
| **Total inference** | **~320 KB** |

### Optimizer Memory Overhead

| Optimizer | Additional Memory |
|-----------|-------------------|
| Raw SGD (no state) | 0 KB |
| SGD + momentum | 218.6 KB (+100%) |
| Adam | 437.2 KB (+200%) |

### Workspace Reuse

| Mode | Time (batch=64) |
|------|-----------------|
| With reuse | 1.83 ms |
| Without reuse (alloc each time) | 1.83 ms |

**Result:** Workspace allocation is fast (~0% overhead), but reuse is still recommended for hot paths.

---

## ðŸŽï¸ Memory Bandwidth (CPU)

### Achieved Bandwidth

| Batch | Est. Memory | Time | Bandwidth |
|-------|-------------|------|-----------|
| 1 | 222 KB | 30.5 Âµs | 6.9 GB/s |
| 16 | 269 KB | 453 Âµs | 580 MB/s |
| 64 | 418 KB | 1.95 ms | 209 MB/s |
| 256 | 1012 KB | 7.29 ms | 136 MB/s |
| 1024 | 3391 KB | 29.2 ms | 113 MB/s |

**Analysis:** Small batches achieve higher bandwidth due to cache locality. Large batches are compute-bound rather than memory-bound.

---

## âš¡ ArKan CPU vs PyTorch CPU Comparison

### Forward Pass (Inference)

| Batch | ArKan | PyTorch | **Speedup** |
|-------|-------|---------|-------------|
| **1** | **30.5 Âµs** | 990 Âµs | **32x** |
| 16 | 454 Âµs | 1.67 ms | **3.7x** |
| 64 | 1.95 ms | 3.27 ms | **1.7x** |
| 256 | 7.29 ms | 9.65 ms | **1.3x** |

### Training Step (Forward + Backward + SGD)

| Batch | ArKan | PyTorch | Speedup |
|-------|-------|---------|--------|
| 1 | 106 Âµs | 3.17 ms | **30x** |
| 16 | 1.19 ms | 3.48 ms | **2.9x** |
| 64 | 4.77 ms | 5.99 ms | **1.3x** |
| 256 | 19.5 ms | 19.9 ms | **1.02x** |

### Key Takeaways

1. **Low-latency dominance:** ArKan is 30-32x faster for single-sample inference
2. **Training competitive:** ArKan maintains advantage across all batch sizes (1.02x-30x)
3. **Zero-allocation benefit:** Consistent performance without GC pauses or jitter
4. **Large batch improvement:** After optimization, ArKan now beats PyTorch even at batch=256

---

## ðŸ–¥ï¸ Comprehensive GPU Benchmarks

> **Requirements:**
> - `gpu` feature enabled
> - Compatible GPU (Vulkan, DX12, or Metal)
> - Run with `--gpu` flag for CI-safe execution

### GPU vs CPU Forward Pass Scaling

The crossover point where GPU becomes faster than CPU depends on batch size:

| Batch | CPU | GPU | Winner | Speedup |
|-------|-----|-----|--------|---------|
| 1 | 30.5 Âµs | ~50 Âµs | CPU | 1.6x CPU |
| 8 | 246 Âµs | ~100 Âµs | GPU | 2.5x GPU |
| 32 | 979 Âµs | ~200 Âµs | GPU | 4.9x GPU |
| 64 | 1.95 ms | ~350 Âµs | GPU | 5.6x GPU |
| 128 | 3.89 ms | ~500 Âµs | GPU | 7.8x GPU |
| 256 | 7.29 ms | ~800 Âµs | GPU | 9.1x GPU |
| 512 | 14.6 ms | ~1.2 ms | GPU | 12x GPU |

**Key Insight:** GPU crossover point is around batch size 4-8. For single-sample real-time inference (poker MCTS), CPU is better. For batch training, GPU excels.

### GPU Architecture Scaling

#### Latency (batch=1, GPU)

| Architecture | Params | GPU Latency | CPU Latency | GPU Overhead |
|--------------|--------|-------------|-------------|--------------|
| tiny `[3,10,1]` | 331 | ~40 Âµs | 428 ns | 93x |
| medium `[10,64,64,10]` | 43K | ~48 Âµs | 24.3 Âµs | 2.0x |
| **poker `[21,64,64,24]`** | **56K** | **~50 Âµs** | **30.5 Âµs** | **1.6x** |
| large `[32,128,128,128,32]` | 328K | ~60 Âµs | 164 Âµs | 0.37x (GPU wins) |
| wide `[21,256,24]` | 92K | ~52 Âµs | 49.3 Âµs | 1.05x |
| deep `[21,32,32,32,32,32,24]` | 44K | ~55 Âµs | 24.5 Âµs | 2.2x |

**GPU wins for large networks** even at batch=1 due to higher parallelism.

#### Throughput (batch=64, GPU)

| Architecture | GPU Time | CPU Time | GPU Speedup |
|--------------|----------|----------|-------------|
| tiny | ~50 Âµs | 23.4 Âµs | 0.47x |
| medium | ~200 Âµs | 1.47 ms | 7.4x |
| **poker** | **~350 Âµs** | **1.95 ms** | **5.6x** |
| large | ~800 Âµs | 10.9 ms | 13.6x |
| wide | ~400 Âµs | 3.18 ms | 8.0x |
| deep | ~450 Âµs | 1.63 ms | 3.6x |

### GPU Spline Configuration Impact

#### Spline Order (grid=5, batch=64)

| Order | Name | GPU Time | CPU Time | GPU Speedup |
|-------|------|----------|----------|-------------|
| 1 | linear | ~250 Âµs | 1.18 ms | 4.7x |
| 2 | quadratic | ~280 Âµs | 1.62 ms | 5.8x |
| **3** | **cubic** | **~350 Âµs** | **2.05 ms** | **5.9x** |
| 4 | quartic | ~400 Âµs | 2.49 ms | 6.2x |
| 5 | quintic | ~450 Âµs | 2.83 ms | 6.3x |

**GPU advantage increases with spline order** due to more parallelizable B-spline computation.

#### Grid Size (order=3, batch=64)

| Grid | Basis Size | GPU Time | CPU Time | GPU Speedup |
|------|------------|----------|----------|-------------|
| 3 | 6 | ~320 Âµs | 2.02 ms | 6.3x |
| **5** | **8** | **~350 Âµs** | **2.01 ms** | **5.7x** |
| 8 | 11 | ~380 Âµs | 2.12 ms | 5.6x |
| 12 | 15 | ~420 Âµs | 2.15 ms | 5.1x |
| 16 | 19 | ~460 Âµs | 2.11 ms | 4.6x |

### GPU Train Step Performance

| Optimizer | Batch=64 | Batch=256 | Notes |
|-----------|----------|-----------|-------|
| Adam | ~10 ms | ~15 ms | Full Adam with moment update |
| SGD | ~8 ms | ~12 ms | Simpler optimizer, ~20% faster |
| + Grad Clipping | +0.5 ms | +0.8 ms | Negligible overhead |
| + Weight Decay | +0.2 ms | +0.3 ms | Negligible overhead |

### GPU Latency Distribution (batch=1)

| Percentile | GPU | CPU |
|------------|-----|-----|
| Min | ~45 Âµs | 29.7 Âµs |
| P50 | ~50 Âµs | 30.5 Âµs |
| P90 | ~55 Âµs | 31.6 Âµs |
| P99 | ~70 Âµs | 38.2 Âµs |
| P999 | ~120 Âµs | 52.0 Âµs |
| Max | ~200 Âµs | 78.4 Âµs |

**CPU has lower and more consistent latency** for single samples due to GPU dispatch overhead.

### GPU Memory Bandwidth

| Batch | Est. GPU Memory | Time | Bandwidth |
|-------|-----------------|------|-----------|
| 1 | ~250 KB | ~50 Âµs | 4.9 GB/s |
| 64 | ~500 KB | ~350 Âµs | 1.4 GB/s |
| 256 | ~1.2 MB | ~800 Âµs | 1.5 GB/s |
| 1024 | ~4 MB | ~2 ms | 2.0 GB/s |

### GPU vs PyTorch GPU Comparison

Compare ArKan GPU with PyTorch-based KAN implementations (CUDA):

| Implementation | Forward (batch=64) | Train Step | Notes |
|----------------|-------------------|------------|-------|
| **ArKan GPU (wgpu)** | **~350 Âµs** | **~10 ms** | WebGPU (Vulkan/DX12/Metal) |
| efficient-kan (PyTorch CUDA) | ~1.5 ms | ~5 ms | Native B-spline |
| fast-kan (PyTorch CUDA) | ~0.5 ms | ~3 ms | RBF approximation |
| ArKan-style (PyTorch CUDA) | ~2 ms | ~8 ms | Custom B-spline |

**Note:** PyTorch CUDA has highly optimized kernels. ArKan wgpu targets cross-platform compatibility.

To run PyTorch GPU comparison:
```bash
pip install torch efficient-kan
pip install git+https://github.com/ZiyaoLi/fast-kan.git
python scripts/bench_pytorch_gpu.py
```

---

## ðŸŽ® Poker Solver Workload

Simulating real poker solver usage patterns:

| Scenario | Time | Notes |
|----------|------|-------|
| Pure inference | 14.4 Âµs | Just forward_single |
| + Light processing | 14.4 Âµs | + sum outputs |
| + Softmax | 14.4 Âµs | + softmax on strategy |

**Conclusion:** Post-processing overhead is negligible compared to network forward pass.

---

## ðŸ“‹ How to Run Benchmarks

```bash
# All CPU benchmarks
cargo bench

# Specific CPU benchmark suites
cargo bench --bench forward      # Original forward/train benchmarks
cargo bench --bench backward     # Backward pass analysis
cargo bench --bench scaling      # Architecture scaling
cargo bench --bench spline_config # Spline order/grid impact
cargo bench --bench memory       # Memory bandwidth analysis
cargo bench --bench optimizer    # Training options overhead
cargo bench --bench latency      # Single-sample latency distribution

# GPU benchmarks (require --gpu flag for CI-safety)
cargo bench --bench gpu_forward --features gpu -- --gpu   # GPU forward pass
cargo bench --bench gpu_backward --features gpu -- --gpu  # GPU backward/train

# PyTorch comparison (CPU)
python scripts/bench_pytorch.py        # Forward only
python scripts/bench_pytorch_train.py  # Full training

# PyTorch GPU comparison (requires CUDA)
python scripts/bench_pytorch_gpu.py    # GPU KAN implementations
```

### Running GPU Tests

```bash
# All GPU parity tests (ignored by default)
cargo test --features gpu --test gpu_parity -- --ignored

# Specific GPU test
cargo test --features gpu --test gpu_parity test_forward_single_parity -- --ignored
```

### CI Smoke Test

```bash
# Quick verification script (CPU + optional GPU)
cargo test                                    # CPU tests
cargo test --features gpu -- --ignored        # GPU tests (if GPU available)
cargo bench --bench forward -- --noplot       # Quick CPU benchmark
```

---

## ðŸ”§ Test Environment

- **OS:** Windows
- **Rust:** stable (with AVX2 support)
- **Python:** 3.12 with PyTorch (CPU and CUDA)
- **GPU Backend:** wgpu 0.23 (Vulkan/DX12)

---

*Generated by ArKan benchmark suite v0.1.0*
