# ArKan Benchmark Results

**Test Date:** December 3, 2025  
**Platform:** Windows, CPU-only  
**Config:** Poker preset `[21, 64, 64, 24]`, Grid 5, Spline Order 3 (cubic)  
**Rust:** `cargo bench` with AVX2/Rayon enabled

---

## üìä Executive Summary

| Metric | Value |
|--------|-------|
| **Single inference latency (P50)** | **30.5 ¬µs** |
| **Single inference throughput** | **~33,000 inferences/sec** |
| **vs PyTorch (batch=1)** | **32x faster** |
| **Memory footprint** | 218.6 KB (weights only) |
| **Zero-allocation training** | ‚úÖ Full train step without allocs |

---

## üéØ Single-Sample Latency (Real-Time Poker)

Critical for MCTS/CFR solvers where thousands of single inferences per second are required.

### Latency Distribution (poker config, batch=1)

| Percentile | Latency |
|------------|---------|
| Min | 29.7 ¬µs |
| **P50 (median)** | **30.5 ¬µs** |
| P90 | 31.6 ¬µs |
| P99 | 38.2 ¬µs |
| P999 | 52.0 ¬µs |
| Max | 78.4 ¬µs |

**Throughput at P50:** ~33,000 inferences/second

### forward_single vs forward_batch(1)

| Method | Time | Notes |
|--------|------|-------|
| `forward_single` | ~15 ¬µs | Optimized single-sample path |
| `forward_batch(1)` | 30.5 ¬µs | Batch overhead visible |

**Recommendation:** Use `forward_single` for real-time play (~2x faster), `forward_batch` for training.

---

## üîÑ Training Performance

### Backward Pass Overhead (batch=64)

| Operation | Time | Overhead vs Forward |
|-----------|------|---------------------|
| forward_only | 1.95 ms | baseline |
| forward_training | 1.95 ms | ~0% (buffer prep is free) |
| **full_train_step** | **4.77 ms** | **+145%** |

**Analysis:** Backward pass takes roughly 2.4x the forward pass time, which is typical for gradient computation. Zero-allocation architecture ensures consistent performance.

### Training Options Impact (batch=64)

| Option | Time | Overhead |
|--------|------|----------|
| No options | 4.77 ms | baseline |
| Gradient clipping (1.0) | 4.59 ms | -3.8% (noise) |
| Weight decay (0.01) | 4.59 ms | -3.8% (noise) |
| Both | 4.67 ms | -2.1% (noise) |

**Conclusion:** Training options have negligible performance impact.

---

## üìê Architecture Scaling

### Latency (batch=1)

| Architecture | Params | Memory | Latency | Throughput |
|--------------|--------|--------|---------|------------|
| tiny `[3,10,1]` | 331 | 1.3 KB | **428 ns** | 7.0 M elem/s |
| medium `[10,64,64,10]` | 43K | 168.5 KB | 24.3 ¬µs | 411 K elem/s |
| **poker `[21,64,64,24]`** | **56K** | **218.6 KB** | **30.5 ¬µs** | **688 K elem/s** |
| large `[32,128,128,128,32]` | 328K | 1.28 MB | 164 ¬µs | 195 K elem/s |
| wide `[21,256,24]` | 92K | 361 KB | 49.3 ¬µs | 426 K elem/s |
| deep `[21,32,32,32,32,32,24]` | 44K | 174 KB | 24.5 ¬µs | 857 K elem/s |

### Throughput (batch=64)

| Architecture | Forward | Train Step |
|--------------|---------|------------|
| tiny | 23.4 ¬µs (8.2 M elem/s) | 61.5 ¬µs (3.1 M elem/s) |
| medium | 1.47 ms (434 K elem/s) | 3.89 ms (165 K elem/s) |
| **poker** | **1.95 ms (689 K elem/s)** | **4.95 ms (271 K elem/s)** |
| large | 10.9 ms (188 K elem/s) | 27.6 ms (74 K elem/s) |
| wide | 3.18 ms (422 K elem/s) | 7.78 ms (173 K elem/s) |
| deep | 1.63 ms (826 K elem/s) | 4.11 ms (327 K elem/s) |

**Insight:** Deep narrow networks (5x32) are faster than wide shallow ones (1x256) for the same parameter count.

---

## üìè Spline Configuration Analysis

### Spline Order Impact (grid=5, batch=64)

| Order | Name | Basis Size | Time | Throughput | Params |
|-------|------|------------|------|------------|--------|
| 1 | linear | 6 | 1.18 ms | **1.14 M elem/s** | 42K |
| 2 | quadratic | 7 | 1.62 ms | 828 K elem/s | 49K |
| **3** | **cubic** | **8** | **2.05 ms** | **656 K elem/s** | **56K** |
| 4 | quartic | 9 | 2.49 ms | 539 K elem/s | 63K |
| 5 | quintic | 10 | 2.83 ms | 475 K elem/s | 70K |

**Trade-off:** Each order increase adds ~400 ¬µs latency but improves function smoothness.

### Grid Size Impact (order=3 cubic, batch=64)

| Grid | Basis Size | Time | Throughput | Params |
|------|------------|------|------------|--------|
| 3 | 6 | 2.02 ms | 665 K elem/s | 42K |
| **5** | **8** | **2.01 ms** | **669 K elem/s** | **56K** |
| 8 | 11 | 2.12 ms | 633 K elem/s | 77K |
| 12 | 15 | 2.15 ms | 624 K elem/s | 105K |
| 16 | 19 | 2.11 ms | 636 K elem/s | 133K |

**Insight:** Grid size has minimal impact on forward speed due to local spline evaluation (only `order+1` basis functions computed). Choose grid size based on required expressiveness.

### Recommended Configurations

| Use Case | Grid | Order | Params | Latency (batch=1) |
|----------|------|-------|--------|-------------------|
| **Fast inference** | 3 | 2 | 35K | ~25 ¬µs |
| **Balanced (default)** | 5 | 3 | 56K | ~28 ¬µs |
| **High accuracy** | 8 | 3 | 77K | ~29 ¬µs |
| **Smooth functions** | 5 | 5 | 70K | ~35 ¬µs |

---

## üíæ Memory Analysis

### Network Memory

| Component | Size (poker config) |
|-----------|---------------------|
| Weights | 218.6 KB |
| Workspace (batch=64) | ~100 KB |
| **Total inference** | **~320 KB** |

### Optimizer Memory Overhead

| Optimizer | Additional Memory |
|-----------|-------------------|
| Raw SGD (no state) | 0 KB |
| SGD + momentum | 218.6 KB (+100%) |
| Adam | 437.2 KB (+200%) |

### Workspace Reuse

| Mode | Time (batch=64) |
|------|-----------------|
| With reuse | 1.83 ms |
| Without reuse (alloc each time) | 1.83 ms |

**Result:** Workspace allocation is fast (~0% overhead), but reuse is still recommended for hot paths.

---

## üèéÔ∏è Memory Bandwidth

### Achieved Bandwidth

| Batch | Est. Memory | Time | Bandwidth |
|-------|-------------|------|-----------|
| 1 | 222 KB | 30.5 ¬µs | 6.9 GB/s |
| 16 | 269 KB | 453 ¬µs | 580 MB/s |
| 64 | 418 KB | 1.95 ms | 209 MB/s |
| 256 | 1012 KB | 7.29 ms | 136 MB/s |
| 1024 | 3391 KB | 29.2 ms | 113 MB/s |

**Analysis:** Small batches achieve higher bandwidth due to cache locality. Large batches are compute-bound rather than memory-bound.

---

## ‚ö° ArKan vs PyTorch Comparison

### Forward Pass (Inference)

| Batch | ArKan | PyTorch | **Speedup** |
|-------|-------|---------|-------------|
| **1** | **30.5 ¬µs** | 990 ¬µs | **32x** |
| 16 | 454 ¬µs | 1.67 ms | **3.7x** |
| 64 | 1.95 ms | 3.27 ms | **1.7x** |
| 256 | 7.29 ms | 9.65 ms | **1.3x** |

### Training Step (Forward + Backward + SGD)

| Batch | ArKan | PyTorch | Speedup |
|-------|-------|---------|--------|
| 1 | 106 ¬µs | 3.17 ms | **30x** |
| 16 | 1.19 ms | 3.48 ms | **2.9x** |
| 64 | 4.77 ms | 5.99 ms | **1.3x** |
| 256 | 19.5 ms | 19.9 ms | **1.02x** |

### Key Takeaways

1. **Low-latency dominance:** ArKan is 30-32x faster for single-sample inference
2. **Training competitive:** ArKan maintains advantage across all batch sizes (1.02x-30x)
3. **Zero-allocation benefit:** Consistent performance without GC pauses or jitter
4. **Large batch improvement:** After optimization, ArKan now beats PyTorch even at batch=256

---

## üéÆ Poker Solver Workload

Simulating real poker solver usage patterns:

| Scenario | Time | Notes |
|----------|------|-------|
| Pure inference | 14.4 ¬µs | Just forward_single |
| + Light processing | 14.4 ¬µs | + sum outputs |
| + Softmax | 14.4 ¬µs | + softmax on strategy |

**Conclusion:** Post-processing overhead is negligible compared to network forward pass.

---

## üìã How to Run Benchmarks

```bash
# All benchmarks
cargo bench

# Specific benchmark suites
cargo bench --bench forward      # Original forward/train benchmarks
cargo bench --bench backward     # Backward pass analysis
cargo bench --bench scaling      # Architecture scaling
cargo bench --bench spline_config # Spline order/grid impact
cargo bench --bench memory       # Memory bandwidth analysis
cargo bench --bench optimizer    # Training options overhead
cargo bench --bench latency      # Single-sample latency distribution

# PyTorch comparison
python scripts/bench_pytorch.py        # Forward only
python scripts/bench_pytorch_train.py  # Full training
```

---

## üîß Test Environment

- **OS:** Windows
- **CPU:** (Your CPU here)
- **Memory:** (Your RAM here)
- **Rust:** stable (with AVX2 support)
- **Python:** 3.12 with PyTorch (CPU)

---

*Generated by ArKan benchmark suite v0.1.0*
